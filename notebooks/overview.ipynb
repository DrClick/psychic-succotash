{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7a8548-b0bf-4da7-9dbf-65756401def0",
   "metadata": {},
   "source": [
    "# Project analysis Composer Classification\n",
    "\n",
    "### Dataset Overview\n",
    "There are 194 files in the **training set**\n",
    "There are 35 files in the **test set**\n",
    "\n",
    "The MIDI files are unique in that they do not directly store audio recordings, they instead represent the score of the music. A musical piece can be composed of multiple instruments with different timings and the MIDI files are organized in layers of instruments each with a timing, a note and a note velocity. It is intended to be played back with a midi synthesizer which reads in each instrument and renders the sound using a specified *“instrument”*. \n",
    "\n",
    "TODO:Piano Roll - Describe the Piano Roll (how it creates a unified score as if played on a single piano. This presents a unified representation for ML where \n",
    "\n",
    "\n",
    "### Challenges / Assumptions\n",
    "Several challenges for this task of 1-vs-All classification as outlined from a 30 second midi file of *live captured audio* recordings\n",
    "The assumption is that this is a live recording of a composition that is being classified in real time\n",
    "1.  Musicians are either using MIDI instruments or a process exists to capture the audio and generate MIDI files\n",
    "2.  There is a limited amount of data, and no given counter examples for the negative class\n",
    "    1.   This could be augmented with an external datasource but the data will be from a different distribution from the process in *1.*\n",
    "    2.   If we augment the data with random midi files of different composers (versus modern classical vs Taylor Swift) it's possible our model will learn to detect a different distribution vs detect the composer and give misleading performance characterists at development time. Clarification with the client is needed to determine the best path forward. \n",
    "4. In the training set there is a large class imbalance for Beethoven. Is is unclear if this is the intended inference-time distribution and this will affect pre-training data processing and system configuration\n",
    "5. No mention of the pipeline system this module should be a part of this will need clairification with the client\n",
    "6. Classical ML methods are preferred which presents a challenge mainly around feature engineering: Features must be manually extracted from the dataset and this will be very difficult. We can create features but domain expertise is likely needed if classical ML is a must have for this project. We need to understand the challenge for the client and the best path forward. It can be argued that CNNs or LSTM are classical in comparison to LLMs. TBD\n",
    "7. The challenge specifically with detecting a composer vs the **audio search**  model (*Shazam*) of detecting pre-recorded music is that for pre-recorded music, there is a [sonic fingerprint](https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf) vs a live recording the score is interpreted by the conductor and each musician. This further complicates the feature engineering. Others have attmpted composer classification directly from the score via the **kern** format [Classifying Musical Scores by Composer:\n",
    "A machine learning approach](https://cs229.stanford.edu/proj2008/LebarChangYu-ClassifyingMusicalScoresByComposer.pdf) but this is not what the client has asked specifically for. To clarify the difficulty the this task, if the Shazam model is using audio fingerprinting, what this task is asking to do is trying to recognize a composer by reading a low fidelity *smudged fingerprint*... An interesting challenge. Again, we need to better understand the challenge the client is trying to solve / success factors and constraints so that we dont overpromise on this project. \n",
    "\n",
    "\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. Clarify Project contraints/success factors with the client. We need to understand what the end goal for the client is in order to be successful\n",
    "2. The current project as defined and available data is likely to be unsuccessful and risks brand reputation for SFL due to client dissatisfaction in performance. If we must deliver a model with the data provided under the assumptions/guidelines given I would recommend passing on this project.\n",
    "\n",
    "### References\n",
    "| Project/Reference | License | Source | Usage Description |\n",
    "|-------------------|---------|--------|-------------------|\n",
    "|Pretty Midi | MIT | https://github.com/craffel/pretty-midi | MIDI file manipulation and data extraction|\n",
    "|Tensorflow | Apache 2.0 | https://github.com/tensorflow/tensorflow/blob/master/LICENSE | Modeling |\n",
    "|SKLearn | BSD 3-Clause | https://github.com/scikit-learn/scikit-learn?tab=BSD-3-Clause-1-ov-file | Modeling |\n",
    "|SKLearn | BSD 3-Clause | https://github.com/scikit-learn/scikit-learn?tab=BSD-3-Clause-1-ov-file | Modeling |\n",
    "\n",
    "\n",
    "\n",
    "### Literature Review\n",
    "| Title | Description | Source  |\n",
    "|-------|-------------|---------|\n",
    "|The Classification of musical scores by composer |Research into the base problem presented by the client, inspiration for *Piano Roll* representation used in these models |https://cs230.stanford.edu/projects_fall_2018/reports/12441334.pdf|\n",
    "|Classifying Musical Scores by Composer |Classical Methods reference |https://cs229.stanford.edu/proj2008/LebarChangYu-ClassifyingMusicalScoresByComposer.pdf]|\n",
    "|Classifying Musical Scores by Composer|Citation Graph |https://www.connectedpapers.com/main/50080fb82064b952d0450e9dced8c9536a129d35/Classifying-Musical-Scores-by-Composer-%3A-A-machine-learning-approach/graph|\n",
    "|Music Genre Classification Using MIDI and Audio Features | Citation Graph |https://www.connectedpapers.com/main/ded47f6f02fd24e752fd8853f2702b43db277904/Music-Genre-Classification-Using-MIDI-and-Audio-Features/graph|\n",
    "|An Industrial-Strength Audio Search Algorithm | Shazam method  |https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568e369-847f-44d6-a1d4-9c525a5cb4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b8d84-740d-4be0-b39d-22ff5930048a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
