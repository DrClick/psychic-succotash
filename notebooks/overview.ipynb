{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7a8548-b0bf-4da7-9dbf-65756401def0",
   "metadata": {},
   "source": [
    "\n",
    "# Project Analysis Composer Classification\n",
    "\n",
    "### Dataset Overview\n",
    "- There are **194 files** in the **training set**.\n",
    "- There are **35 files** in the **test set**.\n",
    "\n",
    "MIDI files are unique as they do not directly store audio recordings. Instead, they represent the *score* of the music. A piece can consist of multiple instruments, each with distinct timings. MIDI files are organized into layers, each representing an instrument with timing, pitch, and velocity information. These files are intended to be played back via a MIDI synthesizer, which interprets the score and renders the sound using specified instruments.\n",
    "\n",
    "Each MIDI file can be represented as a **Piano Roll**, which flattens all instruments into a unified score, as if the entire orchestration is played on a single piano. While this representation trades off fidelity, it is well-suited for ML applications. A Piano Roll is visualized as a 128px x N image, where N is the length of the piece sampled at a frequency \\( f_s \\). Below are example Piano Rolls:\n",
    "\n",
    "| **Bach** | **Holst** |\n",
    "|----------|-----------|\n",
    "|![Example Piano Roll](../resources/piano_roll.png \"Bach's 3rd Cello Suite\")|![Mars Piano Roll](../resources/mars.png \"Holst - Mars\")|\n",
    "\n",
    "These two pieces have clearly distinct fingerprints. The challenge lies in determining if such distinctions are sufficient for building a robust classifier.\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges & Assumptions\n",
    "The task involves 1-vs-All classification from 30-second MIDI clips. Assumptions and challenges include:\n",
    "\n",
    "1. **MIDI Capture Assumption**:\n",
    "   - Musicians use MIDI instruments, or there exists a process to capture audio and convert it into MIDI files.\n",
    "\n",
    "2. **Data Limitation**:\n",
    "   - There is limited data and no counterexamples for the \"unknown\" class. External datasets could augment the negative class, but:\n",
    "     - These would likely come from different distributions (e.g., modern classical or pop music).\n",
    "     - Misalignment between distributions could mislead model evaluation.\n",
    "\n",
    "3. **Class Imbalance**:\n",
    "   - There is a significant class imbalance for Beethoven in the training data. It is unclear if this reflects the inference-time distribution.\n",
    "\n",
    "4. **Undefined System Requirements**:\n",
    "   - The client's desired pipeline and operational use case are unclear. Further clarification is essential.\n",
    "\n",
    "5. **Feature Engineering**:\n",
    "   - Classical ML approaches demand manual feature extraction, requiring significant domain expertise. A CNN/LSTM approach may be more suitable.\n",
    "\n",
    "6. **Interpretation Complexity**:\n",
    "   - Unlike audio search models (e.g., *Shazam*), which rely on [sonic fingerprint](https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf) , this task requires identifying composers based on interpreted, partial, and low-fidelity MIDI data. This adds complexity to feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### Exploratory Data Analysis (EDA) and Initial Modeling\n",
    "Initial modeling used **K-means clustering** and **Gaussian Mixture Models (GMM)**, both with and without kernel tricks. Key steps:\n",
    "\n",
    "- **Dataset Splits**:\n",
    "  - The training set was split into training and validation sets, stratified by composition and grouped by file to prevent overlap.\n",
    "  - Each composition was divided into 20 randomly sampled 30-second slices, ensuring inclusion of the first and last 30 seconds.\n",
    "\n",
    "| **Frame Extraction Visualization** | **Class Distribution** |\n",
    "|-----------------------------------|-----------------------|\n",
    "|![Frame Extraction](../resources/frame_extraction.png \"Frame Extraction\")|![Class Distribution](../resources/class_distribution.png \"Class Distribution\")|\n",
    "\n",
    "- **Feature Selection**:\n",
    "  - Features such as **pitch entropy**, **dominant pitch**, **average velocity**, and **spectral bandwidth** were extracted and visually analyzed.\n",
    "  - Analytical feature selection will be essential for future iterations.\n",
    "\n",
    "- **Clustering Results**:\n",
    "  - K-means clustering was used due to scalability. Cluster centroids were labeled using the mode of known data.\n",
    "\n",
    "| **Train Clusters** | **Validation Clusters** | **Test Clusters** |\n",
    "|--------------------|-------------------------|------------------|\n",
    "|![Train Clusters](../resources/training_clusters.png)|![Validation Clusters](../resources/validation_clusters.png)|![Test Clusters](../resources/test_clusters.png)|\n",
    "\n",
    "#### Evaluation Metrics\n",
    "```\n",
    "Silhouette Score for 4 clusters: 0.2250\n",
    "\n",
    "------------------------------------------------------------\n",
    "Validation Results:\n",
    "predicted_composer  Bach  Beethoven  Brahms  Schubert\n",
    "composer                                             \n",
    "Bach                  44         22       0         0\n",
    "Beethoven              5        589       0         0\n",
    "Brahms                 0         88       0         0\n",
    "Schubert               0        110       0         0\n",
    "\n",
    "Validation Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        Bach       0.90      0.67      0.77        66\n",
    "   Beethoven       0.73      0.99      0.84       594\n",
    "      Brahms       0.00      0.00      0.00        88\n",
    "    Schubert       0.00      0.00      0.00       110\n",
    "\n",
    "    accuracy                           0.74       858\n",
    "   macro avg       0.41      0.41      0.40       858\n",
    "weighted avg       0.57      0.74      0.64       858\n",
    "```\n",
    "\n",
    "The results highlight limitations in classical ML approaches. Further work is needed on feature engineering and dataset augmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### CNN Modeling\n",
    "A CNN was explored as an alternative due to its maturity and suitability for this problem. The architecture:\n",
    "\n",
    "```plaintext\n",
    "Input: (128, 3000, 1)\n",
    "│\n",
    "├── Conv2D(32 filters, 3x3 kernel, ReLU) + L2 Regularization\n",
    "├── MaxPooling2D(2x2)\n",
    "├── Dropout(0.25)\n",
    "│\n",
    "├── Conv2D(64 filters, 3x3 kernel, ReLU) + L2 Regularization\n",
    "├── MaxPooling2D(2x2)\n",
    "├── Dropout(0.25)\n",
    "│\n",
    "├── Flatten\n",
    "├── Dense(64 units, ReLU) + L2 Regularization\n",
    "├── Dropout(0.5)\n",
    "│\n",
    "└── Dense(4 units, Softmax) -> Output Probabilities\n",
    "```\n",
    "\n",
    "#### Training Details:\n",
    "- **Batch Size**: 8, with balanced data generators.\n",
    "- **Optimizer**: Adam (\\( lr=0.001 \\)).\n",
    "- **Loss Function**: Categorical cross-entropy.\n",
    "- **Metrics**: Accuracy, precision, recall.\n",
    "- **Regularization**:\n",
    "  - L2 Regularization (convolutional and dense layers).\n",
    "  - Dropout (to improve generalization).\n",
    "\n",
    "#### Results\n",
    "| Metric               | Value    |\n",
    "|----------------------|----------|\n",
    "| Training Accuracy    | 0.9770   |\n",
    "| Training Loss        | 0.4427   |\n",
    "| Training Precision   | 0.9788   |\n",
    "| Training Recall      | 0.9749   |\n",
    "| Validation Accuracy  | 0.7176   |\n",
    "| Validation Loss      | 1.9245   |\n",
    "| Validation Precision | 0.7246   |\n",
    "| Validation Recall    | 0.6965   |\n",
    "\n",
    "| **Training History** | **AUROC Curve** |\n",
    "|----------------------|-----------------|\n",
    "|![Training History](../resources/training_history.png)|![AUROC Curve](../resources/auroc_curve.png)|\n",
    "\n",
    "These results are promising, suggesting that CNNs are the most viable path forward.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Clarify Project Goals**:\n",
    "   - Engage the client to better understand their desired outcomes and success metrics.\n",
    "\n",
    "2. **Avoid Unrealistic Commitments**:\n",
    "   - The current project scope and data constraints are unlikely to yield satisfactory results. Pursuing this project without clear goals risks client dissatisfaction and harm to SFL's reputation.\n",
    "\n",
    "3. **Iterate on Feature Engineering and Model Choice**:\n",
    "   - If the client insists on classical ML, substantial investment in feature engineering and domain expertise will be required. Alternatively, proceed with CNNs as the primary approach.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "| Project/Reference                         | License      | Source | Usage Description                                    |\n",
    "|-------------------------------------------|--------------|--------|-----------------------------------------------------|\n",
    "| Pretty Midi                                | MIT          | [Link](https://github.com/craffel/pretty-midi)      | MIDI file manipulation and data extraction.        |\n",
    "| TensorFlow                                 | Apache 2.0   | [Link](https://github.com/tensorflow/tensorflow/)   | Deep learning modeling.                            |\n",
    "| scikit-learn                               | BSD 3-Clause | [Link](https://github.com/scikit-learn/scikit-learn)| Classical ML modeling and clustering.              |\n",
    "| Classifying Musical Scores by Composer     | Stanford     | [Link](https://cs229.stanford.edu/projects_fall_2018/reports/12441334.pdf)| Research inspiration for Piano Roll representation.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568e369-847f-44d6-a1d4-9c525a5cb4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b8d84-740d-4be0-b39d-22ff5930048a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
